{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Transformer Networks (GTN)\n",
    "\n",
    "GTN model learning node representation on a **heterogeneous** graph. The model consists of two parts: **Convolutional layer** and **Graph Transformer (GT) layer**. GT layer learns a new meta-path graph via the matrix multiplication of two selected adjacency matrices. The adjacency matrix selection is a weighted sum of candidate adjacency matrices obtained by 1 Ã— 1 convolution. Then, GTN learn node representation via convolution on the learnt meta-path graphs.\n",
    "\n",
    "The following code reproduces the GTN model with dataset ACM. ACM has two GT Layers and other training parameters follow the default settings in the original paper and code. You can download data from this [link](https://drive.google.com/file/d/1qOZ3QjqWMIIvWjzrIdRe3EA4iKzPi6S5/view).\n",
    "\n",
    "In the origin code provided by the author, they also implement a \"sparse\" model, which I think the main difference is to use sparse adjacency matrix to compute the meta path. The following code is the normal model without applying sparse to the adjacency matrix. \n",
    "\n",
    "The result can be found in lab_report in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pdb\n",
    "import pickle\n",
    "import argparse\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, first=True):\n",
    "        super(GTLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.first = first\n",
    "        if self.first == True:\n",
    "            self.gt_conv1 = GTConv(in_channels, out_channels)\n",
    "            self.gt_conv2 = GTConv(in_channels, out_channels)\n",
    "        else:\n",
    "            self.gt_conv = GTConv(in_channels, out_channels)\n",
    "    \n",
    "    def forward(self, A, H_=None):\n",
    "        # The first GTLayer needs two convolutional layers since they will multiply the two matrixes; \n",
    "        # The following GTLayers only need one and will multiply with the before matrix.\n",
    "        if self.first == True:\n",
    "            H = torch.bmm(self.gt_conv1(A),self.gt_conv2(A)) \n",
    "            W = [(F.softmax(self.gt_conv1.weight, dim=1)).detach(),(F.softmax(self.gt_conv1.weight, dim=1)).detach()]\n",
    "        else:\n",
    "            H = torch.bmm(H_, self.gt_conv(A))\n",
    "            W = [(F.softmax(self.gt_conv.weight, dim=1)).detach()]\n",
    "        return H,W\n",
    "\n",
    "# select adjacency matrix: a weighted sum of candidate adjacency matrices obtained by 1 * 1 convolution with non-negative weights from softmax.\n",
    "class GTConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GTConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels \n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels,in_channels,1,1))\n",
    "        self.bias = None\n",
    "        self.scale = nn.Parameter(torch.Tensor([0.1]), requires_grad=False)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        n = self.in_channels\n",
    "        nn.init.constant_(self.weight, 0.1)\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, A):\n",
    "        X = A*F.softmax(self.weight, dim=1)\n",
    "        M = torch.sum(X, dim=1)\n",
    "        return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_edge, num_channels, w_in, w_out, num_class,norm):\n",
    "        super(GTN, self).__init__()\n",
    "        self.num_edge = num_edge\n",
    "        self.num_channels = num_channels\n",
    "        self.w_in = w_in\n",
    "        self.w_out = w_out\n",
    "        self.num_class = num_class\n",
    "        self.is_norm = norm\n",
    "        layers = []\n",
    "        # Two GTLayers\n",
    "        layers.append(GTLayer(num_edge, num_channels, first=True))\n",
    "        layers.append(GTLayer(num_edge, num_channels, first=False))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.weight = nn.Parameter(torch.Tensor(w_in, w_out))\n",
    "        self.bias = nn.Parameter(torch.Tensor(w_out))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.linear1 = nn.Linear(self.w_out*self.num_channels, self.w_out)\n",
    "        self.linear2 = nn.Linear(self.w_out, self.num_class)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "        #GCN model:\n",
    "    def gcn_conv(self,X,H):\n",
    "        X = torch.mm(X, self.weight)\n",
    "        H = self.norm(H, add=True)\n",
    "        return torch.mm(H.t(),X)\n",
    "\n",
    "    def normalization(self, H):\n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                H_ = self.norm(H[i,:,:]).unsqueeze(0)\n",
    "            else:\n",
    "                H_ = torch.cat((H_,self.norm(H[i,:,:]).unsqueeze(0)), dim=0)\n",
    "        return H_\n",
    "\n",
    "    def norm(self, H, add=False):\n",
    "        H = H.t()\n",
    "        if add == False:\n",
    "            H = H*((torch.eye(H.shape[0])==0).type(torch.FloatTensor))\n",
    "        else:\n",
    "            H = H*((torch.eye(H.shape[0])==0).type(torch.FloatTensor)) + torch.eye(H.shape[0]).type(torch.FloatTensor)\n",
    "        deg = torch.sum(H, dim=1)\n",
    "        deg_inv = deg.pow(-1)\n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "        deg_inv = deg_inv*torch.eye(H.shape[0]).type(torch.FloatTensor)\n",
    "        H = torch.mm(deg_inv,H)\n",
    "        H = H.t()\n",
    "        return H\n",
    "\n",
    "    def forward(self, A, X, target_x, target):\n",
    "        A = A.unsqueeze(0).permute(0,3,1,2) \n",
    "        # Apply two GT layers\n",
    "        Ws = []\n",
    "        H, W = self.layers[0](A)\n",
    "        Ws.append(W)\n",
    "        H = self.normalization(H)\n",
    "        H, W = self.layers[1](A, H)\n",
    "        Ws.append(W)\n",
    "        #Apply GCN model to the learned H\n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                X_ = F.relu(self.gcn_conv(X,H[i]))\n",
    "            else:\n",
    "                X_tmp = F.relu(self.gcn_conv(X,H[i]))\n",
    "                X_ = torch.cat((X_,X_tmp), dim=1)\n",
    "        X_ = self.linear1(X_)\n",
    "        X_ = F.relu(X_)\n",
    "        y = self.linear2(X_[target_x])\n",
    "        loss = self.loss(y, target)\n",
    "        return loss, y, Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "node_dim = 64\n",
    "num_channels = 2\n",
    "lr = 0.005\n",
    "weight_decay = 0.001\n",
    "norm = True\n",
    "adaptive_lr = True\n",
    "dataset = 'ACM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data\n",
    "with open('data/'+dataset+'/node_features.pkl','rb') as f:\n",
    "    node_features = pickle.load(f)\n",
    "with open('data/'+dataset+'/edges.pkl','rb') as f:\n",
    "    edges = pickle.load(f)\n",
    "with open('data/'+dataset+'/labels.pkl','rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "num_nodes = edges[0].shape[0]\n",
    "\n",
    "for i,edge in enumerate(edges):\n",
    "    if i ==0:\n",
    "        # using dense matrix\n",
    "        A = torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)\n",
    "    else:\n",
    "        A = torch.cat([A,torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)\n",
    "\n",
    "A = torch.cat([A,torch.eye(num_nodes).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)\n",
    "\n",
    "\n",
    "node_features = torch.from_numpy(node_features).type(torch.FloatTensor)\n",
    "train_node = torch.from_numpy(np.array(labels[0])[:,0]).type(torch.LongTensor)\n",
    "train_target = torch.from_numpy(np.array(labels[0])[:,1]).type(torch.LongTensor)\n",
    "valid_node = torch.from_numpy(np.array(labels[1])[:,0]).type(torch.LongTensor)\n",
    "valid_target = torch.from_numpy(np.array(labels[1])[:,1]).type(torch.LongTensor)\n",
    "test_node = torch.from_numpy(np.array(labels[2])[:,0]).type(torch.LongTensor)\n",
    "test_target = torch.from_numpy(np.array(labels[2])[:,1]).type(torch.LongTensor)\n",
    "\n",
    "num_classes = torch.max(train_target).item()+1\n",
    "\n",
    "#create model:\n",
    "model = GTN(num_edge=A.shape[-1],\n",
    "                    num_channels=num_channels,\n",
    "                    w_in = node_features.shape[1],\n",
    "                    w_out = node_dim,\n",
    "                    num_class=num_classes,\n",
    "                    norm=norm)\n",
    "\n",
    "#using adaptive learning rate:\n",
    "optimizer = torch.optim.Adam([{'params':model.weight},\n",
    "                                {'params':model.linear1.parameters()},\n",
    "                                {'params':model.linear2.parameters()},\n",
    "                                {\"params\":model.layers.parameters(), \"lr\":0.5}\n",
    "                                ], lr=0.005, weight_decay=0.001)\n",
    "#loss:\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Traing:\n",
    "for i in range(epochs):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] > 0.005:\n",
    "            param_group['lr'] = param_group['lr'] * 0.9\n",
    "    print('Epoch:  ',i+1)\n",
    "    model.zero_grad()\n",
    "    model.train()\n",
    "    loss,y_train,Ws = model(A, node_features, train_node, train_target)\n",
    "    train_f1 = torch.tensor(f1_score(torch.argmax(y_train.detach(),dim=1), train_target,  average='macro')).cpu().numpy()\n",
    "    print('Train Loss: {}, Macro_F1: {}'.format(loss.detach().cpu().numpy(), train_f1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()\n",
    "    # Valid and Test every epoch:\n",
    "    with torch.no_grad():\n",
    "        val_loss, y_valid,_ = model.forward(A, node_features, valid_node, valid_target)\n",
    "        val_f1 = torch.tensor(f1_score(torch.argmax(y_valid,dim=1), valid_target, average='macro')).cpu().numpy()\n",
    "        print('Valid Loss: {}, Macro_F1: {}'.format(val_loss.detach().cpu().numpy(), val_f1))\n",
    "        test_loss, y_test,W = model.forward(A, node_features, test_node, test_target)\n",
    "        test_f1 = torch.tensor(f1_score(torch.argmax(y_test,dim=1), test_target, average='macro')).cpu().numpy()\n",
    "        print('Test Loss: {}, Macro_F1: {}\\n'.format(test_loss.detach().cpu().numpy(), test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
